df2[keep, , drop = FALSE]
}
# --- carregar pairs_ranked -------------------------------------------------
pairs_ranked <- read.csv("3_results/pairs_ranked.csv", stringsAsFactors = FALSE, check.names = FALSE)
idA_col <- guess_col(pairs_ranked, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col <- guess_col(pairs_ranked, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col   <- guess_weight_col(pairs_ranked)
if (is.na(idA_col) || is.na(idB_col)) {
stop("Não achei colunas de id em pairs_ranked.csv. Verifique se id_A/id_B vieram do getPairs().")
}
if (is.na(w_col)) stop("pairs_ranked.csv não tem coluna de peso (Weight/score).")
pairs_ranked <- pairs_ranked %>%
rename(id_A = !!idA_col, id_B = !!idB_col, Weight = !!w_col)
# --- histogram & resumo ----------------------------------------------------
p_hist <- ggplot(pairs_ranked, aes(x = Weight)) +
geom_histogram(bins = 40) +
ggtitle("Distribuição dos Pesos de Similaridade (todos os pares)")
ggsave("3_results/hist_pesos.png"), p_hist, width = 7, height = 4, dpi = 120)
ggsave("3_results/hist_pesos.png", p_hist, width = 7, height = 4, dpi = 120)
sum_pairs <- list(
n_pairs         = nrow(pairs_ranked),
n_unique_id_A   = dplyr::n_distinct(pairs_ranked$id_A),
n_unique_id_B   = dplyr::n_distinct(pairs_ranked$id_B),
weight_summary  = summary(pairs_ranked$Weight),
weight_quantile = as.list(quantile(pairs_ranked$Weight, probs = c(.05,.10,.25,.5,.75,.90,.95,.99)))
)
# --- top-1 por id_B (diagnóstico) ------------------------------------------
pairs_top1_byB <- pairs_ranked %>%
group_by(id_B) %>%
slice_max(order_by = Weight, n = 1, with_ties = FALSE) %>%
ungroup()
write.csv(pairs_top1_byB[, c("id_A","id_B","Weight")],"3_results\pairs_ranked_top1.csv", row.names = FALSE)
links_raw <- read.csv("3_results/matches_links.csv", stringsAsFactors = FALSE, check.names = FALSE)
idA2 <- guess_col(links_raw, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB2 <- guess_col(links_raw, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w2   <- guess_weight_col(links_raw)
if (is.na(idA2) || is.na(idB2)) stop("matches_links.csv sem id_A/id_B.")
write.csv(pairs_top1_byB[, c("id_A","id_B","Weight")],"3_results/pairs_ranked_top1.csv", row.names = FALSE)
idA2 <- guess_col(links_raw, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB2 <- guess_col(links_raw, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w2   <- guess_weight_col(links_raw)
if (is.na(idA2) || is.na(idB2)) stop("matches_links.csv sem id_A/id_B.")
if (is.na(w2)) w2 <- "Weight"  # tenta padrão; se não existir, cria peso = NA
if (!w2 %in% names(links_raw)) links_raw$Weight <- NA_real_
links_raw <- links_raw %>% rename(id_A = !!idA2, id_B = !!idB2, Weight = !!w2)
# duplicidades e 1-para-1
dup_byA <- links_raw %>% count(id_A) %>% filter(n > 1) %>% nrow()
dup_byB <- links_raw %>% count(id_B) %>% filter(n > 1) %>% nrow()
links_one2one <- greedy_one2one(links_raw, "id_A", "id_B", "Weight")
write.csv(links_one2one, "3_results/links_one2one_greedy.csv", row.names = FALSE)
# concordância com top-1 por id_B
key_links    <- paste0(links_raw$id_A,"_",links_raw$id_B)
key_links12  <- paste0(links_one2one$id_A,"_",links_one2one$id_B)
key_top1B    <- paste0(pairs_top1_byB$id_A,"_",pairs_top1_byB$id_B)
jacc_raw_top1  <- sum(key_links %in% key_top1B) / length(unique(key_links))
jacc_121_top1  <- sum(key_links12 %in% key_top1B) / length(unique(key_links12))
links_summary <- list(
n_links_raw     = nrow(links_raw),
n_links_one2one = nrow(links_one2one),
dup_groups_id_A = dup_byA,
dup_groups_id_B = dup_byB,
jacc_raw_vs_top1_byB   = jacc_raw_top1,
jacc_1to1_vs_top1_byB  = jacc_121_top1
)
# --- ambiguidade: top-2 muito próximos por id_B -----------------------------
# define "ambíguo" se (top1 - top2) < delta e top1 em zona sensível
delta_gap <- 0.02
zone_min  <- 0.80
zone_max  <- 0.90
top2_byB <- pairs_ranked %>%
group_by(id_B) %>%
slice_max(order_by = Weight, n = 2, with_ties = FALSE) %>%
mutate(rank = row_number()) %>%
ungroup()
amb_tbl <- top2_byB %>%
group_by(id_B) %>%
summarize(
id_A_top1 = id_A[rank==1][1],
w1        = Weight[rank==1][1],
id_A_top2 = ifelse(sum(rank==2)>0, id_A[rank==2][1], NA),
w2        = ifelse(sum(rank==2)>0, Weight[rank==2][1], NA),
gap       = w1 - w2
) %>%
filter(!is.na(w2), w1 >= zone_min, w1 <= zone_max, gap < delta_gap)
write.csv(amb_tbl, "3_results/ambiguous_candidates.csv", row.names = FALSE)
# --- curva de sensibilidade ao limiar (sem truth) --------------------------
W <- pairs_ranked$Weight
grid <- unique(round(seq(quantile(W, 0.05), quantile(W, 0.99), length.out = 25), 3))
curve_df <- lapply(grid, function(t) {
cand <- pairs_ranked %>% filter(Weight >= t) %>% select(id_A, id_B, Weight)
n_cand <- nrow(cand)
uniqA  <- n_distinct(cand$id_A)
uniqB  <- n_distinct(cand$id_B)
# proporção de id_B com >1 candidato acima do limiar (ambiguidade)
amb_B  <- cand %>% count(id_B) %>% summarize(prop = mean(n > 1)) %>% pull(prop)
# tamanho após forçar 1-para-1 greedy
one2one_n <- nrow(greedy_one2one(cand))
data.frame(threshold = t, n_candidates = n_cand, uniq_id_A = uniqA,
uniq_id_B = uniqB, prop_ambiguous_B = amb_B, one2one_size = one2one_n)
})
curve_df <- bind_rows(curve_df)
write.csv(curve_df,"3_results/threshold_curve.csv", row.names = FALSE)
# --- salvar resumo textual --------------------------------------------------
sink("3_results/summary.txt")
cat("==== RESUMO (sem truth) ====\n\n")
cat("PARES (pairs_ranked)\n")
print(sum_pairs)
cat("\nLINKS\n")
print(links_summary)
cat("\nAMBIGUIDADE (top1 vs top2; zona ", zone_min, "-", zone_max,
"; gap <", delta_gap, ")\n", sep = "")
cat("n_ambiguous = ", nrow(amb_tbl), "\n")
cat("\nCurva de limiar salva em threshold_curve.csv\n")
sink()
cat("✔ Avaliação concluída. Arquivos em ", outdir, ":\n",
" - summary.txt\n",
" - hist_pesos.png\n",
" - links_one2one_greedy.csv\n",
" - ambiguous_candidates.csv\n",
" - threshold_curve.csv\n",
" - pairs_ranked_top1.csv\n", sep = "")
library(dplyr)
library(ggplot2)
library(readr)
links_path <- file.path()
pairs_ranked <- read.csv("3_results/linkage/pairs_ranked.csv", stringsAsFactors = FALSE, check.names = FALSE)
truth        <- read.csv("1_raw_data/truth_pairs.csv",  stringsAsFactors = FALSE, check.names = FALSE)
links_raw    <- read.csv("3_results/linkage/matches_links.csv", stringsAsFactors = FALSE, check.names = FALSE)
# --- helpers ---------------------------------------------------------------
guess_col <- function(df, candidates) {
opts <- candidates[candidates %in% names(df)]
if (length(opts) == 0) NA_character_ else opts[1]
}
guess_weight_col <- function(df) {
guess_col(df, c("Weight","weight","score","Score","peso","PESO"))
}
metrics_from_pred <- function(pred_df, truth_df) {
pred_keys  <- paste(pred_df$id_A,  pred_df$id_B,  sep = "_")
truth_keys <- paste(truth_df$id_A, truth_df$id_B, sep = "_")
TP <- sum(pred_keys %in% truth_keys)
FP <- nrow(pred_df) - TP
FN <- sum(!(truth_keys %in% pred_keys))
precision <- ifelse((TP+FP)==0, NA, TP/(TP+FP))
recall    <- ifelse((TP+FN)==0, NA, TP/(TP+FN))
f1        <- ifelse(is.na(precision)|is.na(recall)|(precision+recall)==0, NA, 2*precision*recall/(precision+recall))
tibble(TP, FP, FN, precision, recall, f1, n_pred = nrow(pred_df))
}
# --- padronizar colunas ----------------------------------------------------
idA_col_p <- guess_col(pairs_ranked, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col_p <- guess_col(pairs_ranked, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col_p   <- guess_weight_col(pairs_ranked)
if (is.na(idA_col_p) || is.na(idB_col_p)) stop("pairs_ranked não tem id_A/id_B (verifique saída do getPairs).")
if (is.na(w_col_p)) stop("pairs_ranked não tem Weight/score.")
pairs_ranked <- pairs_ranked |> rename(id_A = !!idA_col_p, id_B = !!idB_col_p, Weight = !!w_col_p)
idA_col_t <- guess_col(truth, c("id_A","id1","id_A_true","idA"))
idB_col_t <- guess_col(truth, c("id_B","id2","id_B_true","idB"))
if (is.na(idA_col_t) || is.na(idB_col_t)) stop("truth_pairs.csv precisa ter colunas id_A e id_B (ou nomes equivalentes).")
truth <- truth |> rename(id_A = !!idA_col_t, id_B = !!idB_col_t) |> distinct(id_A, id_B)
# --- cobertura da blocagem (upper bound do recall) -------------------------
cand_keys  <- paste(pairs_ranked$id_A, pairs_ranked$id_B, sep = "_")
truth_keys <- paste(truth$id_A,       truth$id_B,       sep = "_")
coverage   <- mean(truth_keys %in% cand_keys)  # fração de verdade que apareceu como candidato
writeLines(sprintf("Cobertura da blocagem (verdade presente nos candidatos): %.3f", coverage),
con = file.path(outdir, "coverage.txt"))
# --- histograma de pesos (diagnóstico) -------------------------------------
p_hist <- ggplot(pairs_ranked, aes(x = Weight)) +
geom_histogram(bins = 40) +
ggtitle("Distribuição dos Pesos de Similaridade (pairs_ranked)")
ggsave(file.path(outdir, "hist_pesos.png"), p_hist, width = 7, height = 4, dpi = 120)
# --- DECISÃO A: top-1 por id_B (com e sem limiar) --------------------------
decision_threshold <- 0.85  # ajuste conforme sua calibração
pairs_top1 <- pairs_ranked |>
group_by(id_B) |>
slice_max(order_by = Weight, n = 1, with_ties = FALSE) |>
ungroup() |>
select(id_A, id_B, Weight)
pred_top1_all   <- pairs_top1
pred_top1_thr   <- pairs_top1 |> filter(Weight >= decision_threshold)
m_top1_all <- metrics_from_pred(pred_top1_all, truth) |> mutate(set = "top1_sem_limiar")
m_top1_thr <- metrics_from_pred(pred_top1_thr, truth) |> mutate(set = paste0("top1_weight>=", decision_threshold))
writeLines(sprintf("Cobertura da blocagem (verdade presente nos candidatos): %.3f", coverage),
con = "3_results/coverage.txt")
# --- histograma de pesos (diagnóstico) -------------------------------------
p_hist <- ggplot(pairs_ranked, aes(x = Weight)) +
geom_histogram(bins = 40) +
ggtitle("Distribuição dos Pesos de Similaridade (pairs_ranked)")
ggsave("3_results/hist_pesos.png", p_hist, width = 7, height = 4, dpi = 120)
# --- DECISÃO A: top-1 por id_B (com e sem limiar) --------------------------
decision_threshold <- 0.85  # ajuste conforme sua calibração
pairs_top1 <- pairs_ranked |>
group_by(id_B) |>
slice_max(order_by = Weight, n = 1, with_ties = FALSE) |>
ungroup() |>
select(id_A, id_B, Weight)
pred_top1_all   <- pairs_top1
pred_top1_thr   <- pairs_top1 |> filter(Weight >= decision_threshold)
m_top1_all <- metrics_from_pred(pred_top1_all, truth) |> mutate(set = "top1_sem_limiar")
m_top1_thr <- metrics_from_pred(pred_top1_thr, truth) |> mutate(set = paste0("top1_weight>=", decision_threshold))
bind_rows(m_top1_all, m_top1_thr) |>
write.csv(file.path(outdir, "metrics_top1.csv"), row.names = FALSE)
bind_rows(m_top1_all, m_top1_thr) |>
write.csv("3_results/metrics_top1.csv", row.names = FALSE)
# --- DECISÃO B: links do epiClassify (se houver) ---------------------------
if (!is.null(links_raw)) {
idA_col_l <- guess_col(links_raw, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col_l <- guess_col(links_raw, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col_l   <- guess_weight_col(links_raw); if (is.na(w_col_l)) w_col_l <- "Weight"
if (!w_col_l %in% names(links_raw)) links_raw$Weight <- NA_real_
links <- links_raw |> rename(id_A = !!idA_col_l, id_B = !!idB_col_l, Weight = !!w_col_l) |>
select(id_A, id_B, Weight)
m_links <- metrics_from_pred(links, truth) |> mutate(set = "epiClassify_links")
write.csv(m_links, file.path(outdir, "metrics_epi.csv"), row.names = FALSE)
}
# --- DECISÃO B: links do epiClassify (se houver) ---------------------------
if (!is.null(links_raw)) {
idA_col_l <- guess_col(links_raw, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col_l <- guess_col(links_raw, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col_l   <- guess_weight_col(links_raw); if (is.na(w_col_l)) w_col_l <- "Weight"
if (!w_col_l %in% names(links_raw)) links_raw$Weight <- NA_real_
links <- links_raw |> rename(id_A = !!idA_col_l, id_B = !!idB_col_l, Weight = !!w_col_l) |>
select(id_A, id_B, Weight)
m_links <- metrics_from_pred(links, truth) |> mutate(set = "epiClassify_links")
write.csv("3_results/metrics_epi.csv", row.names = FALSE)
}
# --- PR curve (varrendo limiar em top-1) -----------------------------------
grid <- unique(round(seq(quantile(pairs_top1$Weight, 0.05),
quantile(pairs_top1$Weight, 0.99), length.out = 40), 3))
pr_df <- lapply(grid, function(t) {
pred <- pairs_top1 |> filter(Weight >= t)
cbind(threshold = t, metrics_from_pred(pred, truth))
}) |> bind_rows()
write.csv(pr_df,"3_results/pr_curve.csv", row.names = FALSE)
p_pr <- ggplot(pr_df, aes(x = recall, y = precision)) +
geom_path() + geom_point(size = 1) +
ggtitle("Precision-Recall (varrendo limiar no top-1 por id_B)")
ggsave("3_results/pr_curve.png", p_pr, width = 6, height = 5, dpi = 120)
# --- erros para revisão: FP e FN -------------------------------------------
# Base para checagens (usar top-1 com limiar definido)
pred_keys <- paste(pred_top1_thr$id_A, pred_top1_thr$id_B, sep = "_")
FP_keys   <- setdiff(pred_keys, truth_keys)
FN_keys   <- setdiff(truth_keys, pred_keys)
# FP: pegue do pairs_ranked (tem Weight)
fp_tbl <- pairs_ranked |>
mutate(key = paste(id_A, id_B, sep = "_")) |>
filter(key %in% FP_keys) |>
arrange(desc(Weight)) |>
select(id_A, id_B, Weight)
fn_tbl <- truth |>
mutate(key = paste(id_A, id_B, sep = "_")) |>
filter(key %in% FN_keys) |>
select(id_A, id_B)
write.csv(fp_tbl, "3_results/false_positives.csv", row.names = FALSE)
write.csv(fn_tbl, "3_results/false_negatives.csv", row.names = FALSE)
# --- resumo no console ------------------------------------------------------
cat("\nCobertura (truth presente entre candidatos): ", sprintf("%.3f", coverage), "\n", sep = "")
# --- resumo no console ------------------------------------------------------
cat("\nCobertura (truth presente entre candidatos): ", sprintf("%.3f", coverage), "\n", sep = "")
cat("Métricas (top1):\n"); print(read.csv("3_results/metrics_top1.csv"))
install.packages("pROC")  # se ainda não tiver
library(dplyr)
library(pROC)
library(RecordLinkage)
library(dplyr)
# 1) Leitura -----------------------------------------------------------------
df_A <- read.csv("2_refined_data/dataset_A_pre.csv", stringsAsFactors = FALSE)
df_B <- read.csv("2_refined_data/dataset_B_pre.csv", stringsAsFactors = FALSE)
# 2) Definir campos para blocagem e comparação -------------------------------
cols_block  <- c("municipio_blk", "ano_nasc")                  # blocagem
cols_strcmp <- c("nome", "nome_mae", "data_nascimento")        # comparação aproximada
# Checagem de colunas
miss_A <- setdiff(c(cols_block, cols_strcmp), names(df_A))
miss_B <- setdiff(c(cols_block, cols_strcmp), names(df_B))
if (length(miss_A) > 0) stop("Base A faltando colunas: ", paste(miss_A, collapse = ", "))
if (length(miss_B) > 0) stop("Base B faltando colunas: ", paste(miss_B, collapse = ", "))
# 3) Harmonizar tipos (evita “Data sets have different format”) ---------------
to_char <- function(v) {
if (inherits(v, "Date")) return(format(v, "%Y-%m-%d"))
as.character(v)
}
for (cl in c(cols_block, cols_strcmp)) {
df_A[[cl]] <- to_char(df_A[[cl]])
df_B[[cl]] <- to_char(df_B[[cl]])
}
# Subconjunto nas MESMAS colunas e mesma ordem
common_cols <- c(cols_block, cols_strcmp)
A_use <- df_A[, common_cols]
B_use <- df_B[, common_cols]
# 4) Comparação com blocagem + strcmp ---------------------------------------
rp <- compare.linkage(
A_use, B_use,
blockfld = cols_block,
strcmp   = cols_strcmp
)
cat("\nResumo da comparação (antes dos pesos):\n")
print(summary(rp))
# 5) Pesos, pares e classificação -------------------------------------------
rp <- epiWeights(rp)
# Pares (todos candidatos), ordena por peso
pairs_tbl <- getPairs(rp, single.rows = TRUE)
if (!is.null(pairs_tbl) && nrow(pairs_tbl) > 0) {
pairs_tbl <- pairs_tbl[order(-pairs_tbl$Weight), ]
write.csv(pairs_tbl, "3_results/pairs_ranked.csv", row.names = FALSE)
} else {
warning("getPairs(rp) retornou 0 linhas (nenhum candidato).")
write.csv(data.frame(), "3_results/linkage/pairs_ranked.csv", row.names = FALSE)
}
# Classificação por limiares
threshold_upper <- 0.85
threshold_lower <- 0.60
rp <- epiClassify(rp, threshold.upper = threshold_upper, threshold.lower = threshold_lower)
cat("\nResumo após classificação por limiares:\n")
print(summary(rp))
# 6) LINKS — compatível com várias versões ----------------------------------
# Primeiro tenta com o argumento 'show = "links"'.
links_tbl <- tryCatch(
getPairs(rp, single.rows = TRUE, show = "links"),
error = function(e) NULL
)
# Fallback: filtra manualmente via rp$prediction
if (is.null(links_tbl)) {
pairs_all <- getPairs(rp, single.rows = TRUE)
pred <- rp$prediction
# normaliza 'prediction' (pode ser lógico, numérico, fator ou char)
if (is.factor(pred))    pred <- as.character(pred)
if (is.logical(pred)) {
idx <- pred
} else if (is.numeric(pred)) {
idx <- (pred == 1)
} else {
idx <- tolower(pred) %in% c("l","link","links","match","m","1","true","matched")
}
links_tbl <- pairs_all[idx, , drop = FALSE]
}
# Exporta links (vazio se não houver)
if (!is.null(links_tbl) && nrow(links_tbl) > 0) {
write.csv(links_tbl, "3_results/linkage/matches_links.csv", row.names = FALSE)
} else {
warning("Nenhum link classificado acima do limiar. Arquivo salvo vazio.")
write.csv(data.frame(),"3_results/linkage/matches_links.csv", row.names = FALSE)
}
# Salva o objeto rp para reuso
saveRDS(rp, file = file.path("3_results/linkage/rp_object.rds"))
message("✔ Comparação concluída.")
library(dplyr)
library(ggplot2)
library(readr)
library(pROC)  # para ROC/AUC
# leitura dos datasets
pairs_ranked <- read.csv("3_results/linkage/pairs_ranked.csv", stringsAsFactors = FALSE, check.names = FALSE)
truth        <- read.csv("1_raw_data/truth_pairs.csv",         stringsAsFactors = FALSE, check.names = FALSE)
links_raw    <- read.csv("3_results/linkage/matches_links.csv", stringsAsFactors = FALSE, check.names = FALSE)
# leitura dos datasets
pairs_ranked <- read.csv("3_results/linkage/pairs_ranked.csv", stringsAsFactors = FALSE, check.names = FALSE)
# Pares (todos candidatos), ordena por peso
pairs_tbl <- getPairs(rp, single.rows = TRUE)
# Pares (todos candidatos), ordena por peso
pairs_tbl <- getPairs(rp, single.rows = TRUE)
if (!is.null(pairs_tbl) && nrow(pairs_tbl) > 0) {
pairs_tbl <- pairs_tbl[order(-pairs_tbl$Weight), ]
write.csv(pairs_tbl, "3_results/linkage/pairs_ranked.csv", row.names = FALSE)
} else {
warning("getPairs(rp) retornou 0 linhas (nenhum candidato).")
write.csv(data.frame(), "3_results/linkage/pairs_ranked.csv", row.names = FALSE)
}
library(dplyr)
library(ggplot2)
library(readr)
library(pROC)  # para ROC/AUC
# leitura dos datasets
pairs_ranked <- read.csv("3_results/linkage/pairs_ranked.csv", stringsAsFactors = FALSE, check.names = FALSE)
truth        <- read.csv("1_raw_data/truth_pairs.csv",         stringsAsFactors = FALSE, check.names = FALSE)
links_raw    <- read.csv("3_results/linkage/matches_links.csv", stringsAsFactors = FALSE, check.names = FALSE)
# --- helpers ---------------------------------------------------------------
guess_col <- function(df, candidates) {
opts <- candidates[candidates %in% names(df)]
if (length(opts) == 0) NA_character_ else opts[1]
}
guess_weight_col <- function(df) {
guess_col(df, c("Weight","weight","score","Score","peso","PESO"))
}
metrics_from_pred <- function(pred_df, truth_df) {
pred_keys  <- paste(pred_df$id_A,  pred_df$id_B,  sep = "_")
truth_keys <- paste(truth_df$id_A, truth_df$id_B, sep = "_")
TP <- sum(pred_keys %in% truth_keys)
FP <- nrow(pred_df) - TP
FN <- sum(!(truth_keys %in% pred_keys))
precision <- ifelse((TP+FP)==0, NA, TP/(TP+FP))
recall    <- ifelse((TP+FN)==0, NA, TP/(TP+FN))
f1        <- ifelse(is.na(precision)|is.na(recall)|(precision+recall)==0, NA, 2*precision*recall/(precision+recall))
tibble(TP, FP, FN, precision, recall, f1, n_pred = nrow(pred_df))
}
# --- padronizar colunas ----------------------------------------------------
idA_col_p <- guess_col(pairs_ranked, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col_p <- guess_col(pairs_ranked, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col_p   <- guess_weight_col(pairs_ranked)
if (is.na(idA_col_p) || is.na(idB_col_p)) stop("pairs_ranked não tem id_A/id_B (verifique saída do getPairs).")
if (is.na(w_col_p)) stop("pairs_ranked não tem Weight/score.")
pairs_ranked <- pairs_ranked |> rename(id_A = !!idA_col_p, id_B = !!idB_col_p, Weight = !!w_col_p)
idA_col_t <- guess_col(truth, c("id_A","id1","id_A_true","idA"))
idB_col_t <- guess_col(truth, c("id_B","id2","id_B_true","idB"))
if (is.na(idA_col_t) || is.na(idB_col_t)) stop("truth_pairs.csv precisa ter colunas id_A e id_B (ou nomes equivalentes).")
truth <- truth |> rename(id_A = !!idA_col_t, id_B = !!idB_col_t) |> distinct(id_A, id_B)
# --- cobertura da blocagem (upper bound do recall) -------------------------
cand_keys  <- paste(pairs_ranked$id_A, pairs_ranked$id_B, sep = "_")
truth_keys <- paste(truth$id_A,       truth$id_B,       sep = "_")
coverage   <- mean(truth_keys %in% cand_keys)  # fração de verdade que apareceu como candidato
writeLines(sprintf("Cobertura da blocagem (verdade presente nos candidatos): %.3f", coverage),
con = "3_results/coverage.txt")
# --- histograma de pesos (diagnóstico) -------------------------------------
p_hist <- ggplot(pairs_ranked, aes(x = Weight)) +
geom_histogram(bins = 40) +
ggtitle("Distribuição dos Pesos de Similaridade (pairs_ranked)")
ggsave("3_results/hist_pesos.png", p_hist, width = 7, height = 4, dpi = 120)
# --- DECISÃO A: top-1 por id_B (com e sem limiar) --------------------------
decision_threshold <- 0.85  # ajuste conforme sua calibração
pairs_top1 <- pairs_ranked |>
group_by(id_B) |>
slice_max(order_by = Weight, n = 1, with_ties = FALSE) |>
ungroup() |>
select(id_A, id_B, Weight)
pred_top1_all   <- pairs_top1
pred_top1_thr   <- pairs_top1 |> filter(Weight >= decision_threshold)
m_top1_all <- metrics_from_pred(pred_top1_all, truth) |> mutate(set = "top1_sem_limiar")
m_top1_thr <- metrics_from_pred(pred_top1_thr, truth) |> mutate(set = paste0("top1_weight>=", decision_threshold))
bind_rows(m_top1_all, m_top1_thr) |>
write.csv("3_results/metrics_top1.csv", row.names = FALSE)
# --- DECISÃO B: links do epiClassify (se houver) ---------------------------
if (!is.null(links_raw)) {
idA_col_l <- guess_col(links_raw, c("id_A","id_A.1","id1","id_A_1","id_A.x"))
idB_col_l <- guess_col(links_raw, c("id_B","id_B.2","id2","id_B_2","id_B.y"))
w_col_l   <- guess_weight_col(links_raw); if (is.na(w_col_l)) w_col_l <- "Weight"
if (!w_col_l %in% names(links_raw)) links_raw$Weight <- NA_real_
links <- links_raw |> rename(id_A = !!idA_col_l, id_B = !!idB_col_l, Weight = !!w_col_l) |>
select(id_A, id_B, Weight)
m_links <- metrics_from_pred(links, truth) |> mutate(set = "epiClassify_links")
write.csv(m_links, "3_results/metrics_epi.csv", row.names = FALSE)  # (corrigi só para gravar o objeto)
}
# --- PR curve (varrendo limiar em top-1) -----------------------------------
grid <- unique(round(seq(quantile(pairs_top1$Weight, 0.05),
quantile(pairs_top1$Weight, 0.99), length.out = 40), 3))
pr_df <- lapply(grid, function(t) {
pred <- pairs_top1 |> filter(Weight >= t)
cbind(threshold = t, metrics_from_pred(pred, truth))
}) |> bind_rows()
write.csv(pr_df,"3_results/pr_curve.csv", row.names = FALSE)
p_pr <- ggplot(pr_df, aes(x = recall, y = precision)) +
geom_path() + geom_point(size = 1) +
ggtitle("Precision-Recall (varrendo limiar no top-1 por id_B)")
ggsave("3_results/pr_curve.png", p_pr, width = 6, height = 5, dpi = 120)
# --- erros para revisão: FP e FN -------------------------------------------
# Base para checagens (usar top-1 com limiar definido)
pred_keys <- paste(pred_top1_thr$id_A, pred_top1_thr$id_B, sep = "_")
FP_keys   <- setdiff(pred_keys, truth_keys)
FN_keys   <- setdiff(truth_keys, pred_keys)
# FP: pegue do pairs_ranked (tem Weight)
fp_tbl <- pairs_ranked |>
mutate(key = paste(id_A, id_B, sep = "_")) |>
filter(key %in% FP_keys) |>
arrange(desc(Weight)) |>
select(id_A, id_B, Weight)
fn_tbl <- truth |>
mutate(key = paste(id_A, id_B, sep = "_")) |>
filter(key %in% FN_keys) |>
select(id_A, id_B)
write.csv(fp_tbl, "3_results/false_positives.csv", row.names = FALSE)
write.csv(fn_tbl, "3_results/false_negatives.csv", row.names = FALSE)
# --- curva ROC --------------------------------------------------------------
# Objetivo: avaliar Sensibilidade (TPR) vs Taxa de Falso-Positivo (FPR)
# a) ROC com TODOS os pares candidatos (mais pontos, visão global)
pairs_ranked$.__key <- paste(pairs_ranked$id_A, pairs_ranked$id_B, sep = "_")
y_all <- as.integer(pairs_ranked$.__key %in% truth_keys)   # 1 = par verdadeiro, 0 = não
roc_all <- roc(response = y_all, predictor = pairs_ranked$Weight, quiet = TRUE, direction = ">")
roc_all_df <- data.frame(
threshold = roc_all$thresholds,
TPR = roc_all$sensitivities,                 # sensibilidade = recall
FPR = 1 - roc_all$specificities              # 1 - especificidade
)
write.csv(roc_all_df, "3_results/roc_curve_allpairs.csv", row.names = FALSE)
p_roc_all <- ggplot(roc_all_df, aes(x = FPR, y = TPR)) +
geom_path() +
geom_abline(slope = 1, intercept = 0, linetype = 2) +
ggtitle(paste0("ROC (todos os pares) — AUC = ", round(as.numeric(auc(roc_all)), 4))) +
xlab("Taxa de falso-positivo (FPR)") + ylab("Sensibilidade (TPR)")
ggsave("3_results/roc_curve_allpairs.png", p_roc_all, width = 6, height = 5, dpi = 120)
# b) ROC no TOP-1 por id_B (reflete seu uso final: decisão única por registro B)
y_top1 <- as.integer(paste(pairs_top1$id_A, pairs_top1$id_B, sep = "_") %in% truth_keys)
roc_top1 <- roc(response = y_top1, predictor = pairs_top1$Weight, quiet = TRUE, direction = ">")
roc_top1_df <- data.frame(
threshold = roc_top1$thresholds,
TPR = roc_top1$sensitivities,
FPR = 1 - roc_top1$specificities
)
write.csv(roc_top1_df, "3_results/roc_curve_top1.csv", row.names = FALSE)
p_roc_top1 <- ggplot(roc_top1_df, aes(x = FPR, y = TPR)) +
geom_path() +
geom_abline(slope = 1, intercept = 0, linetype = 2) +
ggtitle(paste0("ROC (top-1 por id_B) — AUC = ", round(as.numeric(auc(roc_top1)), 4))) +
xlab("Taxa de falso-positivo (FPR)") + ylab("Sensibilidade (TPR)")
ggsave("3_results/roc_curve_top1.png", p_roc_top1, width = 6, height = 5, dpi = 120)
# --- resumo no console ------------------------------------------------------
cat("\nCobertura (truth presente entre candidatos): ", sprintf("%.3f", coverage), "\n", sep = "")
cat("Métricas (top1):\n"); print(read.csv("3_results/metrics_top1.csv"))
if (!is.null(links_raw)) { cat("Métricas (epiClassify):\n"); print(read.csv("3_results/metrics_epi.csv")) }
# AUCs para consulta rápida no console
cat(sprintf("AUC-ROC (todos os pares): %.4f\n", as.numeric(auc(roc_all))))
cat(sprintf("AUC-ROC (top-1 por id_B): %.4f\n", as.numeric(auc(roc_top1))))
source("~/oficina_linkage/generate_data.R", echo=TRUE)
source("~/oficina_linkage/generate_data.R", echo=TRUE)
source("~/oficina_linkage/generate_data.R", echo=TRUE)
source("~/oficina_linkage/generate_data.R", echo=TRUE)
source("~/oficina_linkage/generate_data.R", echo=TRUE)
source("~/oficina_linkage/generate_data.R", echo=TRUE)
